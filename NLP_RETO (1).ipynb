{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRSEiOod3_cJ"
   },
   "source": [
    "# Clasificación de emociones en EmoEvent (ES)\n",
    "**Estudio exploratorio de tres modelos (baseline clásico, fine-tuning y zero-shot)**\n",
    "\n",
    "**Dataset:** [EmoEvent](https://github.com/fmplaza/EmoEvent) — 8 clases: `anger, sadness, joy, disgust, fear, surprise, offensive, other`  \n",
    "> En los *splits en español (es)* no suele aparecer `other`. Reportaremos **las etiquetas realmente presentes** en cada split.\n",
    "\n",
    "**Requerimientos del trabajo:**\n",
    "- Explorar **3 modelos** (entrenados, fine-tuning o listos).  \n",
    "- Reportar resultados **por emoción** y **por evento**, incluir **matriz de confusión** y otros análisis pertinentes.  \n",
    "- Entregar **notebook en GitHub** (link al repo).  \n",
    "- Notebook **detallado paso a paso**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lIZ-zuM4Mcu"
   },
   "source": [
    "# Parte 1 — Instalación y Setup\n",
    "\n",
    "**Objetivo:** Dejar el entorno listo para reproducibilidad, descargar el dataset y fijar estructura de carpetas.\n",
    "\n",
    "**En esta sección haremos:**\n",
    "1) Verificar GPU/versión de Python.  \n",
    "2) Instalar dependencias (`transformers`, `datasets`, `sklearn`, utilidades de limpieza).  \n",
    "3) Clonar **EmoEvent** y cargar los *splits* en español (`train/dev/test`).  \n",
    "4) Crear carpetas de salida y fijar semillas para reproducibilidad.\n",
    "\n",
    ">  Si actualizas paquetes con la sesión ya abierta, puede ser necesario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "lFisjmTkpQMB",
    "outputId": "b764fda0-26cc-49e2-e30e-8e593acfc80a"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L || echo \"Sin GPU visible\"\n",
    "!python --version\n",
    "!pip -q install pandas numpy scikit-learn matplotlib seaborn emoji clean-text[gpl] unidecode datasets evaluate accelerate \"transformers==4.55.4\"\n",
    "\n",
    "from pathlib import Path\n",
    "import random, numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "!rm -rf /content/EmoEvent\n",
    "!git clone --depth 1 https://github.com/fmplaza/EmoEvent.git /content/EmoEvent\n",
    "\n",
    "ES_DIR = Path(\"/content/EmoEvent/splits/es\")\n",
    "assert (ES_DIR/\"train.tsv\").exists() and (ES_DIR/\"dev.tsv\").exists() and (ES_DIR/\"test.tsv\").exists()\n",
    "read_tsv = lambda p: pd.read_csv(p, sep=\"\\t\", dtype=str).fillna(\"\")\n",
    "df_train = read_tsv(ES_DIR/\"train.tsv\")\n",
    "df_dev   = read_tsv(ES_DIR/\"dev.tsv\")\n",
    "df_test  = read_tsv(ES_DIR/\"test.tsv\")\n",
    "\n",
    "BASE_DIR = Path(\"/content\")\n",
    "DATA_DIR = ES_DIR\n",
    "OUT_DIR  = BASE_DIR / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"SEED:\", SEED)\n",
    "print(\"Shapes (train, dev, test):\", df_train.shape, df_dev.shape, df_test.shape)\n",
    "print(\"Columnas:\", list(df_train.columns))\n",
    "display(df_train.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF-QUQqp83dt"
   },
   "source": [
    "# Parte 2 — EDA (Análisis Exploratorio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jJTgBjGj83Mx",
    "outputId": "8f90a9fc-51c2-41d8-eb24-d5124266a0f8"
   },
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "ALL_LABELS_8 = ['anger','sadness','joy','disgust','fear','surprise','offensive','other']\n",
    "\n",
    "def add_label8(df):\n",
    "    d = df.copy()\n",
    "    off = d[\"offensive\"].str.upper().str.strip()\n",
    "    emo = d[\"emotion\"].str.lower().str.strip().replace({\"others\":\"other\"})\n",
    "    d[\"label8\"] = np.where(off==\"OFF\",\"offensive\",emo)\n",
    "    d = d[d[\"label8\"].isin(ALL_LABELS_8)].reset_index(drop=True)\n",
    "    return d\n",
    "\n",
    "df_train = add_label8(df_train)\n",
    "df_dev   = add_label8(df_dev)\n",
    "df_test  = add_label8(df_test)\n",
    "\n",
    "LABELS = sorted(pd.concat([df_train[\"label8\"], df_dev[\"label8\"], df_test[\"label8\"]]).unique())\n",
    "print(\"Etiquetas:\", LABELS)\n",
    "\n",
    "def dist_tbl(df, col):\n",
    "    vc = df[col].value_counts()\n",
    "    return pd.DataFrame({\"count\":vc, \"pct\":vc/vc.sum()})\n",
    "\n",
    "print(\"Distribución por clase (train)\"); display(dist_tbl(df_train,\"label8\"))\n",
    "print(\"Distribución por clase (dev)\");   display(dist_tbl(df_dev,\"label8\"))\n",
    "print(\"Distribución por clase (test)\");  display(dist_tbl(df_test,\"label8\"))\n",
    "\n",
    "for name, d in [(\"TRAIN\",df_train),(\"DEV\",df_dev),(\"TEST\",df_test)]:\n",
    "    print(name,\"nulls\"); display(d[[\"id\",\"event\",\"tweet\",\"offensive\",\"emotion\",\"label8\"]].isna().sum().to_frame(\"nulls\"))\n",
    "    d[\"_len_char\"] = d[\"tweet\"].str.len()\n",
    "    d[\"_len_tok\"]  = d[\"tweet\"].str.split().apply(len)\n",
    "    print(name,\"duplicados por id:\", d[\"id\"].duplicated().sum(), \"por tweet:\", d[\"tweet\"].duplicated().sum())\n",
    "    display(d[[\"_len_char\",\"_len_tok\"]].describe().T)\n",
    "\n",
    "leak_td = set(df_train[\"tweet\"]).intersection(set(df_dev[\"tweet\"]))\n",
    "leak_tt = set(df_train[\"tweet\"]).intersection(set(df_test[\"tweet\"]))\n",
    "leak_dt = set(df_dev[\"tweet\"]).intersection(set(df_test[\"tweet\"]))\n",
    "print(\"Fugas entre splits (tweets): train-dev:\",len(leak_td),\"train-test:\",len(leak_tt),\"dev-test:\",len(leak_dt))\n",
    "\n",
    "url_re  = re.compile(r'http\\S+|www\\.\\S+')\n",
    "at_re   = re.compile(r'@\\w+')\n",
    "hash_re = re.compile(r'#\\w+')\n",
    "emoji_re = re.compile(r'[\\U0001F300-\\U0001FAFF]')\n",
    "\n",
    "def tweet_signals(df):\n",
    "    s = df[\"tweet\"]\n",
    "    return pd.DataFrame({\n",
    "        \"has_url\":[s.str.contains(url_re).mean()],\n",
    "        \"has_@\":[s.str.contains(at_re).mean()],\n",
    "        \"has_#\":[s.str.contains(hash_re).mean()],\n",
    "        \"has_emoji\":[s.str.contains(emoji_re).mean()],\n",
    "    }).T.rename(columns={0:\"ratio\"})\n",
    "\n",
    "print(\"Señales en TRAIN\"); display(tweet_signals(df_train))\n",
    "\n",
    "rows=[]\n",
    "for lab, g in df_train.groupby(\"label8\"):\n",
    "    sig = tweet_signals(g).assign(label=lab).reset_index().rename(columns={\"index\":\"signal\"})\n",
    "    rows.append(sig)\n",
    "sig_by_label = pd.concat(rows, ignore_index=True)\n",
    "print(\"Señales por clase (TRAIN)\"); display(sig_by_label)\n",
    "\n",
    "sns.set_theme()\n",
    "vc = df_train[\"label8\"].value_counts()\n",
    "plt.figure(figsize=(6,3)); sns.barplot(x=vc.index, y=vc.values); plt.title(\"Distribución de clases (train)\"); plt.xticks(rotation=45); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3)); sns.histplot(df_train[\"_len_tok\"], bins=40); plt.title(\"Longitud en tokens (train)\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "ct = pd.crosstab(df_train[\"event\"], df_train[\"label8\"], normalize=\"index\")\n",
    "plt.figure(figsize=(9,9)); sns.heatmap(ct, cmap=\"Blues\"); plt.title(\"Evento × clase (proporción, train)\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "def sample_by_label(df, n=3, seed=42):\n",
    "    out=[]\n",
    "    for lab, g in df.groupby(\"label8\", sort=True):\n",
    "        k = min(n,len(g))\n",
    "        if k>0:\n",
    "            out.append(g.sample(n=k, random_state=seed)[[\"event\",\"label8\",\"tweet\"]])\n",
    "    return pd.concat(out).reset_index(drop=True)\n",
    "\n",
    "print(\"Muestrario por clase (train)\"); display(sample_by_label(df_train, n=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AoV-nU5BGOj"
   },
   "source": [
    "# Parte 2 — Limpieza de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "8yVHwnnQBIcj",
    "outputId": "27b37320-3cbd-471c-fb1c-8f7ac93ef1a6"
   },
   "outputs": [],
   "source": [
    "import re, unicodedata, pandas as pd, numpy as np\n",
    "\n",
    "URL_RE   = re.compile(r'http\\S+|www\\.\\S+')\n",
    "AT_RE    = re.compile(r'@\\w+')\n",
    "HASH_RE  = re.compile(r'#')\n",
    "RT_RE    = re.compile(r'\\bRT\\b:?')\n",
    "USER_RE  = re.compile(r'\\bUSER\\b', flags=re.IGNORECASE)\n",
    "WS_RE    = re.compile(r'\\s+')\n",
    "PUNKS_RE = re.compile(r'([!?¡¿.,;:])\\1{2,}')\n",
    "LAUGH_RE = re.compile(r'\\b(ja|je|ji|jo|ju){2,}\\b|jaja+|jeje+|jiji+|jojo+|juju+|x+d+', re.IGNORECASE)\n",
    "LETTER_ELONG_RE = re.compile(r'([A-Za-zÁÉÍÓÚÜÑáéíóúüñ])\\1{2,}')\n",
    "EMOJI_RE = re.compile(r'[\\U00002600-\\U000026FF\\U00002700-\\U000027BF\\U0001F1E0-\\U0001F1FF\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FAFF]', flags=re.UNICODE)\n",
    "EMOTICON_RE = re.compile(r'(:-?\\)|:-?\\(|;-?\\)|:-?D|xD|XD|:-?P|<3|:\\')', re.IGNORECASE)\n",
    "\n",
    "ES_STOPWORDS = {\n",
    "    'a','acá','ahí','al','algo','alguna','algunas','alguno','algunos','allá','alli','allí','ante','antes','aquel','aquella','aquellas','aquello','aquellos','aquí','así','aun','aún','aunque','bajo','bien','cada','casi','como','con','contra','cual','cuales','cualquier','cualquiera','cualquieras','cuan','cuándo','cuanto','cuantos','de','del','desde','donde','dos','el','él','ella','ellas','ello','ellos','en','entre','era','erais','eran','eras','eres','es','esa','esas','ese','eso','esos','esta','estaba','estabais','estaban','estabas','estad','estada','estadas','estado','estados','estamos','estando','estar','estaremos','estará','estarán','estarás','estaré','estaréis','estaría','estaríais','estaríamos','estarían','estarías','estas','este','estemos','esto','estos','estoy','estuve','estuviera','estuvierais','estuvieran','estuvieras','estuvieron','estuviese','estuvieseis','estuviesen','estuvieses','estuvimos','estuviste','estuvisteis','estuvo','fin','fue','fuera','fuerais','fueran','fueras','fueron','fuese','fueseis','fuesen','fueses','fui','fuimos','ha','habida','habidas','habido','habidos','habiendo','habremos','habrá','habrán','habrás','habré','habréis','habría','habríais','habríamos','habrían','habrías','han','has','hasta','hay','haya','hayamos','hayan','hayas','he','hemos','hube','hubiera','hubierais','hubieran','hubieras','hubieron','hubiese','hubieseis','hubiesen','hubieses','hubimos','hubiste','hubisteis','hubo','la','las','le','les','lo','los','mas','más','me','mi','mis','mismo','mucha','muchas','mucho','muchos','muy','nada','nos','nosotras','nosotros','otra','otras','otro','otros','para','pero','poco','por','porque','que','qué','quien','quién','quienes','quienesquiera','quienquiera','se','sea','seamos','sean','seas','seremos','será','serán','serás','seré','seréis','sería','seríais','seríamos','serían','serías','si','sí','sido','siempre','so','sobre','sois','solamente','solo','somos','son','su','sus','suya','suyas','suyo','suyos','tanto','te','teneis','tenéis','tenemos','tengo','tener','tendrá','tendrán','tendrás','tendré','tendréis','tendría','tendríais','tendríamos','tendrían','tendrías','ti','tiene','tienen','tienes','todo','todos','tu','tus','tuya','tuyas','tuyo','tuyos','un','una','uno','unos','vosotras','vosotros','vuestra','vuestras','vuestro','vuestros','ya','y'\n",
    "}\n",
    "CRUCIALES = {'no','nunca','jamás','sin','ni','tampoco','nadie','ninguno','ninguna','ningún','nada'}\n",
    "STOPWORDS = ES_STOPWORDS - CRUCIALES\n",
    "\n",
    "def _norm(s):\n",
    "    return unicodedata.normalize('NFC', str(s))\n",
    "\n",
    "def _tok_filter(tokens):\n",
    "    out=[]\n",
    "    for t in tokens:\n",
    "        if not t:\n",
    "            continue\n",
    "        if t in STOPWORDS:\n",
    "            continue\n",
    "        if len(t)==1 and t not in {'no','y','o'}:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "def clean_remove_emoji_stopwords(text):\n",
    "    s = _norm(text)\n",
    "    s = URL_RE.sub(' ', s)\n",
    "    s = AT_RE.sub(' ', s)\n",
    "    s = HASH_RE.sub('', s)\n",
    "    s = RT_RE.sub(' ', s)\n",
    "    s = USER_RE.sub(' ', s)\n",
    "    s = EMOJI_RE.sub(' ', s)\n",
    "    s = EMOTICON_RE.sub(' ', s)\n",
    "    s = PUNKS_RE.sub(r'\\1\\1', s)\n",
    "    s = LAUGH_RE.sub(' ', s)\n",
    "    s = LETTER_ELONG_RE.sub(r'\\1\\1', s)\n",
    "    s = s.lower()\n",
    "    s = WS_RE.sub(' ', s).strip()\n",
    "    toks = s.split()\n",
    "    toks = _tok_filter(toks)\n",
    "    s = ' '.join(toks).strip()\n",
    "    return s if s else \"<empty>\"\n",
    "\n",
    "for d in (df_train, df_dev, df_test):\n",
    "    d[\"text_clean\"] = d[\"tweet\"].apply(clean_remove_emoji_stopwords)\n",
    "\n",
    "print(\"vacíos:\", (df_train.text_clean==\"<empty>\").sum(), (df_dev.text_clean==\"<empty>\").sum(), (df_test.text_clean==\"<empty>\").sum())\n",
    "display(df_train[[\"tweet\",\"text_clean\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1faxru4EjiZ"
   },
   "source": [
    "# Parte 3 — Modelo 1: TF-IDF + LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ed2jdAxrEiwc",
    "outputId": "aed8f76d-6271-448b-f104-ace33850ed64"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "\n",
    "OUT_M1 = Path(\"/content/outputs/m1\"); OUT_M1.mkdir(parents=True, exist_ok=True)\n",
    "LABELS = sorted(pd.concat([df_train[\"label8\"], df_dev[\"label8\"], df_test[\"label8\"]]).unique())\n",
    "\n",
    "def metrics_by_label(y_true, y_pred, labels=LABELS):\n",
    "    pr, rc, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)\n",
    "    return pd.DataFrame({'label':labels,'precision':pr,'recall':rc,'f1':f1,'support':sup})\n",
    "\n",
    "def event_macro_f1(df_pred):\n",
    "    rows=[]\n",
    "    for ev, g in df_pred.groupby('event'):\n",
    "        rows.append({'event':ev,'macro_f1':f1_score(g['label8'], g['pred'], labels=LABELS, average='macro', zero_division=0),'n':len(g)})\n",
    "    return pd.DataFrame(rows).sort_values('macro_f1', ascending=False)\n",
    "\n",
    "def make_model(C=1.0):\n",
    "    return Pipeline([\n",
    "        ('feats', FeatureUnion([\n",
    "            ('word', TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, sublinear_tf=True, max_features=50000)),\n",
    "            ('char', TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, sublinear_tf=True, max_features=100000)),\n",
    "        ])),\n",
    "        ('svm', LinearSVC(C=C, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "\n",
    "X_tr, y_tr = df_train[\"text_clean\"].tolist(), df_train[\"label8\"].tolist()\n",
    "X_dev, y_dev = df_dev[\"text_clean\"].tolist(), df_dev[\"label8\"].tolist()\n",
    "X_te,  y_te  = df_test[\"text_clean\"].tolist(), df_test[\"label8\"].tolist()\n",
    "\n",
    "best_C, best_score = None, -1\n",
    "for C in [0.5, 1.0, 2.0, 4.0]:\n",
    "    m = make_model(C); m.fit(X_tr, y_tr)\n",
    "    pred_dev = m.predict(X_dev)\n",
    "    f1m = f1_score(y_dev, pred_dev, labels=LABELS, average='macro', zero_division=0)\n",
    "    if f1m > best_score: best_C, best_score = C, f1m\n",
    "\n",
    "final = make_model(best_C)\n",
    "final.fit(X_tr+X_dev, y_tr+y_dev)\n",
    "pred_te = final.predict(X_te)\n",
    "\n",
    "print(\"Mejor C (dev):\", best_C, \"| F1-macro(dev):\", round(best_score,4))\n",
    "print(classification_report(y_te, pred_te, labels=LABELS, digits=3, zero_division=0))\n",
    "\n",
    "per_label = metrics_by_label(y_te, pred_te); per_label.to_csv(OUT_M1/'per_label.csv', index=False)\n",
    "dfp = df_test[['event','tweet','label8']].copy(); dfp['pred'] = pred_te\n",
    "per_event = event_macro_f1(dfp); per_event.to_csv(OUT_M1/'per_event.csv', index=False)\n",
    "\n",
    "cm = confusion_matrix(y_te, pred_te, labels=LABELS)\n",
    "cmn = confusion_matrix(y_te, pred_te, labels=LABELS, normalize='true')\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(7,5)); sns.heatmap(cm, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS); plt.title(\"M1: Confusión (conteos)\"); plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.tight_layout(); plt.savefig(OUT_M1/'confusion_counts.png', dpi=140); plt.show()\n",
    "plt.figure(figsize=(7,5)); sns.heatmap(cmn, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS); plt.title(\"M1: Confusión (normalizada)\"); plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.tight_layout(); plt.savefig(OUT_M1/'confusion_norm.png', dpi=140); plt.show()\n",
    "\n",
    "m1_macro = f1_score(y_te, pred_te, labels=LABELS, average='macro', zero_division=0)\n",
    "m1_weighted = f1_score(y_te, pred_te, labels=LABELS, average='weighted', zero_division=0)\n",
    "pd.DataFrame([{'model':'TFIDF+LinearSVC','f1_macro_test':m1_macro,'f1_weighted_test':m1_weighted,'best_C':best_C}]).to_csv(OUT_M1/'summary.csv', index=False)\n",
    "display(per_label.sort_values('support', ascending=False).reset_index(drop=True))\n",
    "display(per_event.head(10))\n",
    "print(\"Guardado en:\", OUT_M1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g1IKj3CGeLz"
   },
   "source": [
    "Conclusión (M1)\n",
    "\n",
    "El macro-F1 ≈ 0.19 es bajo porque el promedio por clase castiga las minoritarias (fear, disgust, surprise), que casi siempre se confunden con clases grandes.\n",
    "\n",
    "El modelo rinde mejor en offensive (F1≈0.56) y other (F1≈0.52) y, en menor medida, en joy (F1≈0.24); sadness queda bajo (F1≈0.14).\n",
    "\n",
    "La matriz de confusión muestra una deriva hacia other, típica del desbalance y de rasgos léxicos poco específicos tras limpiar emojis/stopwords.\n",
    "\n",
    "Por evento, el macro-F1 varía: WorldBookDay y Venezuela están arriba; GretaThunberg y tweets sin evento, abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QIl4gj5AGlxu",
    "outputId": "d926fce8-8935-4791-ef6a-ec5748d9667e"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sns.set_theme()\n",
    "OUT = Path(\"/content/outputs/m1\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Asegurar tablas\n",
    "if 'per_label' not in globals():\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    pr, rc, f1, sup = precision_recall_fscore_support(y_te, pred_te, labels=LABELS, zero_division=0)\n",
    "    per_label = pd.DataFrame({'label':LABELS,'precision':pr,'recall':rc,'f1':f1,'support':sup})\n",
    "if 'per_event' not in globals():\n",
    "    dfp = df_test[['event','label8']].copy(); dfp['pred'] = pred_te\n",
    "    rows=[]\n",
    "    from sklearn.metrics import f1_score\n",
    "    for ev, g in dfp.groupby('event'):\n",
    "        rows.append({'event':ev,'macro_f1':f1_score(g['label8'], g['pred'], labels=LABELS, average='macro', zero_division=0),'n':len(g)})\n",
    "    per_event = pd.DataFrame(rows).sort_values('macro_f1', ascending=False)\n",
    "\n",
    "# 1) F1 por emoción (ordenado)\n",
    "plt.figure(figsize=(7,4))\n",
    "ax = sns.barplot(data=per_label.sort_values('f1', ascending=False), x='f1', y='label')\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.2f\", padding=3)\n",
    "plt.title(\"M1: F1 por emoción (test)\")\n",
    "plt.xlabel(\"F1\"); plt.ylabel(\"Emoción\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_f1_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "# 2) Precisión vs Recall por emoción\n",
    "df_pr = per_label.melt(id_vars=['label','support'], value_vars=['precision','recall'], var_name='metric', value_name='score')\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=df_pr, x='label', y='score', hue='metric')\n",
    "plt.title(\"M1: Precisión y Recall por emoción (test)\")\n",
    "plt.ylim(0,1); plt.xlabel(\"Emoción\"); plt.ylabel(\"Score\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_precision_recall_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "# 3) Soporte por emoción\n",
    "plt.figure(figsize=(7,3))\n",
    "ax = sns.barplot(data=per_label.sort_values('support', ascending=False), x='label', y='support')\n",
    "ax.bar_label(ax.containers[0], padding=2)\n",
    "plt.title(\"M1: Soporte por emoción (test)\")\n",
    "plt.xlabel(\"Emoción\"); plt.ylabel(\"# ejemplos\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_support_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "# 4) Macro-F1 por evento (Top 12 y Bottom 12 con n>=20)\n",
    "pe = per_event.copy()\n",
    "pe_ge20 = pe[pe['n']>=20]\n",
    "topk = pe_ge20.head(12); botk = pe_ge20.tail(12)\n",
    "\n",
    "plt.figure(figsize=(8,4.2))\n",
    "sns.barplot(data=topk, x='macro_f1', y='event')\n",
    "plt.title(\"M1: Macro-F1 por evento (Top 12, n>=20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_event_top12.png\", dpi=140); plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4.2))\n",
    "sns.barplot(data=botk, x='macro_f1', y='event')\n",
    "plt.title(\"M1: Macro-F1 por evento (Bottom 12, n>=20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_event_bottom12.png\", dpi=140); plt.show()\n",
    "\n",
    "# 5) Matriz de confusión normalizada (anotada)\n",
    "cmn = confusion_matrix(y_te, pred_te, labels=LABELS, normalize='true')\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cmn, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS, annot=False)\n",
    "plt.title(\"M1: Confusión (normalizada)\")\n",
    "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m1_confusion_norm_annot.png\", dpi=140); plt.show()\n",
    "\n",
    "print(\"Figuras guardadas en:\", OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laV7ZBh7HAvt"
   },
   "source": [
    "## Conclusión — Modelo 1: TF-IDF + LinearSVC\n",
    "\n",
    "**Resumen de desempeño (test)**  \n",
    "- Accuracy ≈ 0.37, F1-macro ≈ **0.19**, F1-weighted ≈ 0.36.  \n",
    "- Mejor por clase: **offensive (F1≈0.56)** y **other (F1≈0.53)**.  \n",
    "- Intermedias: **joy (F1≈0.24)**, **sadness (F1≈0.14)**.  \n",
    "- Muy bajas: **anger (F1≈0.07)**, **surprise (F1≈0.02)**, **fear/disgust ≈ 0**.\n",
    "\n",
    "**Interpretación de la matriz de confusión**  \n",
    "- Fuerte **deriva hacia “other”**: muchas instancias reales de anger/fear/disgust/surprise terminan predichas como “other”.  \n",
    "- También hay desplazamientos hacia **offensive** y **joy**, lo que sugiere que las pistas léxicas del texto limpio favorecen clases frecuentes y generalistas.\n",
    "\n",
    "**Análisis por evento (macro-F1)**  \n",
    "- Mejor: **WorldBookDay (~0.21)**, **Venezuela (~0.20)**, **ChampionsLeague (~0.19)**.  \n",
    "- Peor: **GretaThunberg (~0.11)** y tweets sin evento (~0.09).  \n",
    "- La variación por evento indica **mezclas de clase y vocabulario específicos**; donde domina “other” o hay pocas muestras de emociones minoritarias el macro-F1 cae.\n",
    "\n",
    "**Impacto de la limpieza**  \n",
    "- Se removieron **emojis** y **stopwords**. En emociones esto reduce señales afectivas y contexto sintáctico, lo que probablemente **perjudicó recall** en clases minoritarias (fear, disgust, surprise).\n",
    "\n",
    "**Limitaciones del enfoque**  \n",
    "- **Desbalance severo** (other y joy muy frecuentes) + modelo lineal con n-gramas → sesgo hacia clases mayoritarias.  \n",
    "- Falta de **contexto semántico** e ironía/sarcasmo; TF-IDF no capta bien matices.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusión**  \n",
    "El baseline es **competente en clases grandes** pero **insuficiente** para emociones minoritarias. Para cumplir el objetivo del trabajo se requiere un modelo contextual (BETO) y estrategias de balanceo; además, mantener señales afectivas (como emojis) debería mejorar el reconocimiento de emociones sutiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0gkE22DHrAD"
   },
   "source": [
    "# Parte 4 — Modelo 2: BETO (fine-tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ccd6873cad09440ab8d38c15b13f9390",
      "b46463ed9b064a31a36e385c8cef84d0",
      "f5d5d2b274b648e983c74511da56832b",
      "ddc2be1f8b7748e99cbf68da6810c5e1",
      "bf23e9d405a1462e8acbbd2c8791772b",
      "ad0130b968104b0889d53438c07d8fad",
      "5c44ecbdeb8649b0a740b7f9a147e8f7",
      "030e5836d39d4cbcb60d16a5cef8e396",
      "cfb73d52eeaa4109ae3fc64f7eb8c9b2",
      "4c5b652d90dd4d71b85c25629921fc1d",
      "29a6e66e47524fc18a7424217b951d6e",
      "e83be8b87afb4a299ff7625255ef4737",
      "27ad598b59204392afabd8b81c4b72c2",
      "d5376215a58745089495a12c72b304cd",
      "ff7699eb029c4f53be6cb34c33d300c5",
      "c0516ca9471e442c89c3e34e250fc957",
      "440f960d5017488f91710ddc62b4c780",
      "c2e2097947a54be5a4aee13383fb5e69",
      "a1557a2f3f0f42acb72b9568c4e5dd1f",
      "e72b9e1cc8264240a6c012210f2b083a",
      "8f4789392c54402985ba235d4fc0f13d",
      "c52cb0db2b514977a3d1fb8eb5af8b04",
      "b1fdcac9de504ae681200143b7a8e743",
      "b1fddfd9c9fc4c7593aec0ae646b70a0",
      "24d95b6fdbac41ed9d54f1b3bf96c8d5",
      "b4a19d65589e4dbbb553e1f2b491976a",
      "4a691ce8e5b848c2b22dbb5af79ec373",
      "e0ff952d6e1d4991880fb6a44ad95e7e",
      "26e829d7c47f4d0badf1e32809c971fd",
      "d8c3e943dfb3450e87161ada0684bc25",
      "b6d58f2bfde14d0fa5d343e6e0cb5c49",
      "f6a033baa2774dca87d38354f4b8ab7f",
      "31c6a932c4364a37a7c73312ef73965c"
     ]
    },
    "id": "EYruEW9DHtHQ",
    "outputId": "8339d9e0-1a53-4b63-b351-46a827e708c7"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets evaluate accelerate\n",
    "\n",
    "import re, unicodedata, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "                          DataCollatorWithPadding, EarlyStoppingCallback)\n",
    "\n",
    "OUT_M2 = Path(\"/content/outputs/m2\"); OUT_M2.mkdir(parents=True, exist_ok=True)\n",
    "sns.set_theme()\n",
    "\n",
    "URL_RE=re.compile(r'http\\S+|www\\.\\S+'); AT_RE=re.compile(r'@\\w+'); HASH_RE=re.compile(r'#'); RT_RE=re.compile(r'\\bRT\\b:?'); USER_RE=re.compile(r'\\bUSER\\b', re.I); WS_RE=re.compile(r'\\s+')\n",
    "def _norm(s): return unicodedata.normalize('NFC', str(s))\n",
    "def clean_light(s):\n",
    "    s=_norm(s); s=URL_RE.sub(' ',s); s=AT_RE.sub(' @user ',s); s=HASH_RE.sub('',s); s=RT_RE.sub(' ',s); s=USER_RE.sub('@user',s); s=WS_RE.sub(' ',s).strip()\n",
    "    return s if s else \"<empty>\"\n",
    "\n",
    "for d in (df_train, df_dev, df_test):\n",
    "    d[\"text_beto\"] = d[\"tweet\"].apply(clean_light)\n",
    "\n",
    "LABELS = sorted(pd.concat([df_train[\"label8\"], df_dev[\"label8\"], df_test[\"label8\"]]).unique())\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "def to_hfds(df):\n",
    "    return Dataset.from_dict({\"text\":df[\"text_beto\"].tolist(), \"labels\":[label2id[x] for x in df[\"label8\"]]})\n",
    "ds_tr, ds_va, ds_te = to_hfds(df_train), to_hfds(df_dev), to_hfds(df_test)\n",
    "\n",
    "MODEL = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "def tk(b): return tok(b[\"text\"], truncation=True, max_length=160)\n",
    "tr_tok = ds_tr.map(tk, batched=True, remove_columns=[\"text\"])\n",
    "va_tok = ds_va.map(tk, batched=True, remove_columns=[\"text\"])\n",
    "te_tok = ds_te.map(tk, batched=True, remove_columns=[\"text\"])\n",
    "coll = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(LABELS), id2label=id2label, label2id=label2id)\n",
    "\n",
    "cls_w = compute_class_weight(class_weight=\"balanced\", classes=np.array(LABELS), y=df_train[\"label8\"])\n",
    "cls_w_t = torch.tensor(cls_w, dtype=torch.float, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    yt = [id2label[i] for i in labels]\n",
    "    yp = [id2label[i] for i in preds]\n",
    "    return {\"f1_macro\": f1_score(yt, yp, labels=LABELS, average=\"macro\", zero_division=0),\n",
    "            \"f1_weighted\": f1_score(yt, yp, labels=LABELS, average=\"weighted\", zero_division=0)}\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = self.loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_M2/\"trainer\"),\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "args.eval_strategy = \"epoch\"\n",
    "args.save_strategy = \"epoch\"\n",
    "args.load_best_model_at_end = True\n",
    "args.metric_for_best_model = \"f1_macro\"\n",
    "args.greater_is_better = True\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=cls_w_t,\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tr_tok,\n",
    "    eval_dataset=va_tok,\n",
    "    processing_class=tok,\n",
    "    data_collator=coll,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "pred = trainer.predict(te_tok)\n",
    "\n",
    "pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "y_pred = [id2label[i] for i in pred_ids]\n",
    "y_true = [id2label[i] for i in te_tok[\"labels\"]]\n",
    "\n",
    "pr, rc, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=LABELS, zero_division=0)\n",
    "per_label = pd.DataFrame({\"label\":LABELS,\"precision\":pr,\"recall\":rc,\"f1\":f1,\"support\":sup})\n",
    "per_label.to_csv(OUT_M2/\"per_label.csv\", index=False)\n",
    "\n",
    "cmn = confusion_matrix(y_true, y_pred, labels=LABELS, normalize=\"true\")\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cmn, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.title(\"M2 BETO: Confusión (normalizada)\"); plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.savefig(OUT_M2/\"confusion_norm.png\", dpi=140); plt.show()\n",
    "\n",
    "rows=[]\n",
    "for ev, g in df_test.assign(pred=y_pred)[[\"event\",\"label8\",\"pred\"]].groupby(\"event\"):\n",
    "    rows.append({\"event\":ev,\"macro_f1\":f1_score(g[\"label8\"], g[\"pred\"], labels=LABELS, average=\"macro\", zero_division=0),\"n\":len(g)})\n",
    "per_event = pd.DataFrame(rows).sort_values(\"macro_f1\", ascending=False)\n",
    "per_event.to_csv(OUT_M2/\"per_event.csv\", index=False)\n",
    "\n",
    "f1m = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\", zero_division=0)\n",
    "f1w = f1_score(y_true, y_pred, labels=LABELS, average=\"weighted\", zero_division=0)\n",
    "pd.DataFrame([{\"model\":\"BETO-FT\",\"f1_macro_test\":f1m,\"f1_weighted_test\":f1w}]).to_csv(OUT_M2/\"summary.csv\", index=False)\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels=LABELS, digits=3, zero_division=0))\n",
    "print(\"F1-macro:\", round(f1m,4), \"| F1-weighted:\", round(f1w,4))\n",
    "print(\"Guardado en:\", OUT_M2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50Ii46UiMZMM"
   },
   "source": [
    "## Conclusión — Modelo 2: BETO (fine-tuning)\n",
    "\n",
    "**Resumen (test)**  \n",
    "- **F1-macro = 0.217**, **F1-weighted = 0.347**, accuracy ≈ 0.336.  \n",
    "- Mejores clases: **offensive (F1≈0.67)**; mejora clara vs. baseline.  \n",
    "- Intermedias: **joy (F1≈0.26)**, **other (F1≈0.47)** con menor recall que en el baseline.  \n",
    "- Muy bajas: **anger/sadness** siguen débiles y **fear/disgust/surprise** casi nulas (se confunden con `other` o `joy`).\n",
    "\n",
    "**Comparación con el Modelo 1 (TF-IDF+SVM)**  \n",
    "- **+ F1-macro**: sube de ~0.19 → **~0.22** (pequeña mejora global).  \n",
    "- **+ offensive** sube notablemente; **joy** también mejora.  \n",
    "- **≈/− other** pierde recall, lo que sugiere que el modelo deja de “ir a lo seguro” con `other`, pero aún no aprende bien las minoritarias.\n",
    "\n",
    "**Gap dev → test**  \n",
    "- En validación (dev) reportaste F1-macro por época ~0.42→0.49, pero en test cae a **0.22**: indica **cambio de distribución** entre dev y test y/o **sobreajuste** al conjunto dev.\n",
    "\n",
    "\n",
    "**Conclusión**  \n",
    "BETO mejora el baseline, pero el **desbalance y el shift dev→test** limitan el F1-macro. Con **balanceo + Focal Loss + evento en el input** y un **encoder más robusto en español**, debería aumentar el recall de minoritarias y, con ello, el F1-macro global.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5_QeP4GOMqTL",
    "outputId": "21f81013-da7f-41d3-8f7f-8cd5e4f03dd1"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, f1_score\n",
    "\n",
    "sns.set_theme()\n",
    "OUT = Path(\"/content/outputs/m2\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'y_true' not in globals() or 'y_pred' not in globals():\n",
    "    raise RuntimeError(\"Faltan y_true/y_pred del Modelo 2.\")\n",
    "\n",
    "if 'per_label' not in globals():\n",
    "    pr, rc, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=LABELS, zero_division=0)\n",
    "    per_label = pd.DataFrame({'label':LABELS,'precision':pr,'recall':rc,'f1':f1,'support':sup})\n",
    "\n",
    "if 'per_event' not in globals():\n",
    "    dfp = df_test[['event','label8']].copy(); dfp['pred'] = y_pred\n",
    "    rows=[]\n",
    "    for ev, g in dfp.groupby('event'):\n",
    "        rows.append({'event':ev,'macro_f1':f1_score(g['label8'], g['pred'], labels=LABELS, average='macro', zero_division=0),'n':len(g)})\n",
    "    per_event = pd.DataFrame(rows).sort_values('macro_f1', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "ax = sns.barplot(data=per_label.sort_values('f1', ascending=False), x='f1', y='label')\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.2f\", padding=3)\n",
    "plt.title(\"M2: F1 por emoción (test)\"); plt.xlabel(\"F1\"); plt.ylabel(\"Emoción\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_f1_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "df_pr = per_label.melt(id_vars=['label','support'], value_vars=['precision','recall'], var_name='metric', value_name='score')\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=df_pr, x='label', y='score', hue='metric')\n",
    "plt.title(\"M2: Precisión y Recall por emoción (test)\")\n",
    "plt.ylim(0,1); plt.xlabel(\"Emoción\"); plt.ylabel(\"Score\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_precision_recall_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "ax = sns.barplot(data=per_label.sort_values('support', ascending=False), x='label', y='support')\n",
    "ax.bar_label(ax.containers[0], padding=2)\n",
    "plt.title(\"M2: Soporte por emoción (test)\")\n",
    "plt.xlabel(\"Emoción\"); plt.ylabel(\"# ejemplos\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_support_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "pe = per_event.copy()\n",
    "pe_ge20 = pe[pe['n']>=20]\n",
    "topk = pe_ge20.head(12); botk = pe_ge20.tail(12)\n",
    "\n",
    "plt.figure(figsize=(8,4.2))\n",
    "sns.barplot(data=topk, x='macro_f1', y='event')\n",
    "plt.title(\"M2: Macro-F1 por evento (Top 12, n>=20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_event_top12.png\", dpi=140); plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4.2))\n",
    "sns.barplot(data=botk, x='macro_f1', y='event')\n",
    "plt.title(\"M2: Macro-F1 por evento (Bottom 12, n>=20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_event_bottom12.png\", dpi=140); plt.show()\n",
    "\n",
    "cmn = confusion_matrix(y_true, y_pred, labels=LABELS, normalize='true')\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cmn, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.title(\"M2: Matriz de confusión (normalizada)\")\n",
    "plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m2_confusion_norm.png\", dpi=140); plt.show()\n",
    "\n",
    "print(\"Figuras guardadas en:\", OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUTw8yvBNJGL"
   },
   "source": [
    "## Conclusión — Modelo 2 (BETO fine-tuning)\n",
    "\n",
    "**Desempeño global (test):** F1-macro ≈ **0.22**, F1-weighted ≈ **0.35**. Supone una **mejora leve** frente al baseline TF-IDF+SVM (≈0.19 macro-F1), pero aún insuficiente.\n",
    "\n",
    "**Por emoción:**\n",
    "- **offensive** es la mejor clase (**F1≈0.67**), con buena precisión y recall.\n",
    "- **other** se mantiene aceptable (**F1≈0.47**) aunque pierde recall respecto al baseline.\n",
    "- **joy** mejora moderadamente (**F1≈0.26**).\n",
    "- **sadness/anger** siguen bajos (**0.14 / 0.09**).\n",
    "- **surprise/fear/disgust** prácticamente no despegan (**≤0.06**), reflejando fuerte desbalance y falta de señales.\n",
    "\n",
    "**Matriz de confusión:**\n",
    "- Persisten derivas hacia **other** y, en menor medida, **joy**; p.ej., muchos **fear** reales se predicen como **joy**, y **anger/surprise** como **other**.\n",
    "- El modelo reduce la “comodidad” con *other* (más repartido que en M1), pero **no logra capturar** las minoritarias.\n",
    "\n",
    "**Por evento:**\n",
    "- Macro-F1 por evento es bajo y **heterogéneo** (≈0.10–0.22). `GameOfThrones` y `GretaThunberg` quedan arriba; `NotreDame` y otros, abajo.\n",
    "- El rendimiento varía con el **léxico del evento**, lo que sugiere dependencia del dominio y mezcla de clases por evento.\n",
    "\n",
    "**Lectura general:**\n",
    "- BETO aporta **más capacidad contextual** y mejora `offensive/joy`, pero el **desbalance severo** y el **shift dev→test** limitan el F1-macro.\n",
    "- Las clases raras carecen de masa crítica y señales fuertes; el modelo sigue “tirando” a clases frecuentes.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusión:** El fine-tuning con BETO **mejora ligeramente** el macro-F1 y acierta bien `offensive`, pero sigue **fallando en minoritarias**. Con **balanceo**, **pérdida focal** y **evento como señal**, debería subir el recall de emociones raras y, con ello, el F1-macro global.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynBSryzdbw4a"
   },
   "source": [
    "## Modelo 3 (Zero-shot NLI con τ ajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2109ab6a7a79480a9963785939d47b0c",
      "7f1ceea874ea43b888fff966e26124e1",
      "2dc74c36d2a44323a20d705cc4f6f386",
      "442976c177ce4a1b93e44838e71e5432",
      "1522c8bf489540a7a4d1e8267f48a6fd",
      "a3a2087696e94e6a8e66ce816560cfec",
      "4137823cda644fb9a63fdab796228dbe",
      "e90b66c8cb654df29101dae624eee7dc",
      "14481cf9b50d4cf8b5fd3aaee508cde6",
      "c0cf60bd21cb401f96733e6c74922873",
      "c3c1aaa1d7054216ad1ae60be131acd5",
      "d4828713f75048ce85e0a521ed1b8a2f",
      "94de00e7b65b420f8e89bbfe9eef7c44",
      "88d784cb116a4d43af166dd7b30f84e1",
      "fb0a08cc1489452aa4e843632183726d",
      "86d4d37b80d34afdabde3ea83c495d42",
      "c66fd3d685e143ae922a470f4a7028e6",
      "a36e1ac47f3b4ce18ec306106e72ebbd",
      "911bb4ffce044143b6ba9a294199155e",
      "68c4e5bfeea94acca1513606bfa23d7e",
      "eb564a296b544eecba4a195b5e2718dd",
      "347a5ab20a9d429a8ef576f6c56a63d7",
      "d62d16d57a7147369b5d9541a38c603f",
      "28fe3f17ab364793993cb94c883d1046",
      "bbf8e0b157ae44f5be3a5e67c272bb52",
      "e21aa9cc63984f79802d455b0c0f2dae",
      "021a79a89434438ba3dc40d56de52205",
      "472acd88ec404bcf9f883419678e481c",
      "a899be948d0848c4936744fa1b277d13",
      "74292233339042acba0cd58e6e2f211b",
      "18f05c0db2bc461484562bb2697dab70",
      "92503d6b142040a4b6a8109d97b65964",
      "19c01f9237854e9aad7d53e26690c6bb",
      "34e204479bc2454cb1151a39cffe4703",
      "042b7e3b5a9b4130a9e756ebea95628e",
      "bf2f94ede7c44110b488ccd469921856",
      "66988c16935d45f4b15c845d6d69a69c",
      "81403dbeeca8455485150a1ca42cf680",
      "1e747587b2034627b1bacedf90563ca2",
      "e305733dfa88428fa7ad75e6c8754a77",
      "31f5e882ba1b41d9b62ba976f58cc034",
      "5a2868032cbe4da8b7b82c1aaae52fb6",
      "c35cc352bfed46c3bed69e1843736ee1",
      "47085a88bada4ea391d106631650cae1",
      "5d638a5c032f4c049d0f273e429da726",
      "154bd8c1c56b443f92f88dec06b108f0",
      "3730ebfdcfbc4612bad5b2b672211351",
      "7bb39860f210472c9b27f058f0554588",
      "c60408f94e06463a88f52a2b5a539dc5",
      "66f9ceb2ec8d472a8d3f8f81e1906e16",
      "00f4ac3b08094bb2993728220afb29e8",
      "e789fce3fc1346d697a587ca4222924c",
      "2c6256b559b3470f8502c7a5830faa79",
      "d86af60644f54f5ba05d08b043e4c03f",
      "7ce58071c6c9497cb240d0994afaf446",
      "2bb7c29a641f4360a6b8796ada7e2023",
      "81dc98a4c3054169a17a74759b9829b5",
      "8760bd21cc7241dcb8ef13010a070462",
      "43e468f217aa4b65a713135fa195902d",
      "104fd13bf32c463985b3d730e6bc85ca",
      "04ade0c518bc41fa975e0f3bfd9e4fcc",
      "d74601f50efb48d98302bb3d571f281b",
      "5a3e36abe47e42198ef2c66878ced057",
      "df7ebc8fdaab415587016c000c2189bb",
      "677b918248444c768e7f19ca3e355f6b",
      "fbd2cc8074294a0bbdec3b1b21d34102",
      "6238f79c8acf46668195064f37fd8403",
      "615a00efb165437d93f10193b808e931",
      "cd1d50c6e5a04c69b15823780448389c",
      "51b62467d81a4b60bcd0d618f534afcd",
      "69f7b1a7772149069a604d3ce20c5f8d",
      "bbe4d11ed79b4b429f5992ab877013a2",
      "be0655c9f7c2487eadf649a2770bc736",
      "ad9bce9e32374228937cbd493c578a9e",
      "66f8dcb370204e468e69b5434ab8794d",
      "f00d48e276e242a584ecdcd35c35c13c",
      "951f02ff15ed44f1b0a4e78b5fe8526a",
      "58e195eb69d74229945514f5bc9ec647",
      "8f7e6252a9f1492883a446ec53e09bfa",
      "a7772e241ae942f5a609e35216d57aad",
      "5bd2f8131cdc42818706d3a98f7cdd67",
      "e6c388effef747ae9313a3a56467d433",
      "ae8e72455b22447681a164ce953ecf12",
      "cb13c91d26c042ae959f67324f6e4511",
      "5218af4afc4e49089d269726d97fc37d",
      "da37e25ce37942209acb152d0f9af9bd",
      "91459a4c130840049c0c23bd39fe8c7b",
      "89bdaab35db845a89035995c9ff7173b",
      "f9bf1fdfcc8b4425979c3ce728caad09",
      "758f4852dd114974b551ada451285259",
      "f9f037335201418c9a28e56416b506e2",
      "dcc79cefffb94c349a2277b1c5887499",
      "53cea13052034aa8a4419bfd7c551406",
      "6a7995d8c5cd456cac89d3a26ac3c450",
      "3a41952be5644781aee630a14f2b19b0",
      "297dc19e59084d36ad6e30ceb10c38e0",
      "5d73ad3a065a413f9749f29aa748227c",
      "e2e884a8c4764457b4c7e98cbecf1a17",
      "d23c73f2cc8645a5bad819b017e65cd1"
     ]
    },
    "id": "7GbZX1HIN155",
    "outputId": "8eaa09e3-039a-4670-86c1-b8435f8da1fb"
   },
   "outputs": [],
   "source": [
    "# === Zero-shot NLI (mDeBERTa) — carga robusta y warm-up, luego inferencia ===\n",
    "import os, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "sns.set_theme()\n",
    "OUT = Path(\"/content/outputs/m3_tuned\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "LABELS = sorted(pd.concat([df_train[\"label8\"], df_dev[\"label8\"], df_test[\"label8\"]]).unique())\n",
    "LABELS_NO_OTHER = [l for l in LABELS if l != \"other\"]\n",
    "verbalizers = {\"anger\":\"enojo\",\"sadness\":\"tristeza\",\"joy\":\"alegría\",\"disgust\":\"asco\",\"fear\":\"miedo\",\"surprise\":\"sorpresa\",\"offensive\":\"lenguaje ofensivo\"}\n",
    "cand_phrases = [verbalizers[l] for l in LABELS_NO_OTHER]\n",
    "phrase2label = {v:k for k,v in verbalizers.items()}\n",
    "HYP = \"Este texto expresa {}.\"\n",
    "\n",
    "MODEL = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "\n",
    "def build_pipeline(device):\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "    dtype = torch.float16 if (device==0 and torch.cuda.is_available()) else torch.float32\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(MODEL, torch_dtype=dtype, low_cpu_mem_usage=True)\n",
    "    if device==0: mdl.to(\"cuda\")\n",
    "    zsc = pipeline(\"zero-shot-classification\", model=mdl, tokenizer=tok, device=device)\n",
    "    return zsc\n",
    "\n",
    "try:\n",
    "    zsc = build_pipeline(DEVICE)\n",
    "    _ = zsc(\"Prueba rápida\", candidate_labels=cand_phrases, hypothesis_template=HYP, multi_label=False)\n",
    "except Exception as e:\n",
    "    print(\"Fallo en GPU, usando CPU:\", e)\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    zsc = build_pipeline(-1)\n",
    "\n",
    "def predict_labels(texts, tau=0.45, batch=8):\n",
    "    preds, scores = [], []\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"Zero-shot tuned\"):\n",
    "        out = zsc(texts[i:i+batch], candidate_labels=cand_phrases, hypothesis_template=HYP, multi_label=False)\n",
    "        if isinstance(out, dict): out = [out]\n",
    "        for r in out:\n",
    "            p, s = r[\"labels\"][0], float(r[\"scores\"][0])\n",
    "            lab = phrase2label.get(p, \"other\")\n",
    "            if s < tau: lab = \"other\"\n",
    "            preds.append(lab); scores.append(s)\n",
    "    return preds, scores\n",
    "\n",
    "def eval_split(df, tau=0.45, batch=8):\n",
    "    texts = [f\"Evento: {e}. Tweet: {t}\" if str(e).strip() else f\"Tweet: {t}\" for e,t in zip(df[\"event\"], df[\"tweet\"])]\n",
    "    y_pred, s = predict_labels(texts, tau=tau, batch=batch)\n",
    "    y_true = df[\"label8\"].tolist()\n",
    "    f1m = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\", zero_division=0)\n",
    "    f1w = f1_score(y_true, y_pred, labels=LABELS, average=\"weighted\", zero_division=0)\n",
    "    return y_true, y_pred, s, f1m, f1w\n",
    "\n",
    "taus = np.round(np.linspace(0.30, 0.60, 7), 2)\n",
    "best_tau, best_f1m = None, -1\n",
    "for tau in taus:\n",
    "    _, _, _, f1m_dev, _ = eval_split(df_dev, tau=tau, batch=8)\n",
    "    if f1m_dev > best_f1m:\n",
    "        best_f1m, best_tau = f1m_dev, tau\n",
    "\n",
    "y_true_dev, y_pred_dev, _, _, _ = eval_split(df_dev, tau=best_tau, batch=8)\n",
    "print(\"Mejor τ (dev):\", best_tau, \"| F1-macro(dev):\", round(best_f1m,4))\n",
    "print(classification_report(y_true_dev, y_pred_dev, labels=LABELS, digits=3, zero_division=0))\n",
    "\n",
    "y_true, y_pred, scores, f1m, f1w = eval_split(df_test, tau=best_tau, batch=8)\n",
    "print(\"F1-macro(test):\", round(f1m,4), \"| F1-weighted(test):\", round(f1w,4))\n",
    "print(classification_report(y_true, y_pred, labels=LABELS, digits=3, zero_division=0))\n",
    "\n",
    "df_pred = df_test.copy(); df_pred[\"pred\"]=y_pred; df_pred[\"score\"]=scores\n",
    "df_pred.to_csv(OUT/\"predictions.csv\", index=False)\n",
    "\n",
    "pr, rc, f1, sup = precision_recall_fscore_support(y_true, y_pred, labels=LABELS, zero_division=0)\n",
    "per_label = pd.DataFrame({\"label\":LABELS,\"precision\":pr,\"recall\":rc,\"f1\":f1,\"support\":sup})\n",
    "per_label.to_csv(OUT/\"per_label.csv\", index=False)\n",
    "\n",
    "rows=[]\n",
    "for ev, g in df_pred.groupby(\"event\"):\n",
    "    rows.append({\"event\":ev, \"macro_f1\":f1_score(g[\"label8\"], g[\"pred\"], labels=LABELS, average=\"macro\", zero_division=0), \"n\":len(g)})\n",
    "per_event = pd.DataFrame(rows).sort_values(\"macro_f1\", ascending=False)\n",
    "per_event.to_csv(OUT/\"per_event.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "ax = sns.barplot(data=per_label.sort_values(\"f1\", ascending=False), x=\"f1\", y=\"label\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.2f\", padding=3)\n",
    "plt.title(\"M3 (zero-shot, τ ajustado): F1 por emoción\"); plt.xlabel(\"F1\"); plt.ylabel(\"Emoción\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m3_f1_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "df_pr = per_label.melt(id_vars=[\"label\",\"support\"], value_vars=[\"precision\",\"recall\"], var_name=\"metric\", value_name=\"score\")\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=df_pr, x=\"label\", y=\"score\", hue=\"metric\")\n",
    "plt.title(\"M3: Precisión vs. Recall por emoción\"); plt.ylim(0,1); plt.xlabel(\"Emoción\"); plt.ylabel(\"Score\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m3_precision_recall_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "ax = sns.barplot(data=per_label.sort_values(\"support\", ascending=False), x=\"label\", y=\"support\")\n",
    "ax.bar_label(ax.containers[0], padding=2)\n",
    "plt.title(\"M3: Soporte por emoción\"); plt.xlabel(\"Emoción\"); plt.ylabel(\"# ejemplos\"); plt.xticks(rotation=45); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m3_support_per_label.png\", dpi=140); plt.show()\n",
    "\n",
    "cmn = confusion_matrix(y_true, y_pred, labels=LABELS, normalize=\"true\")\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cmn, cmap=\"Blues\", xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.title(\"M3: Matriz de confusión (normalizada)\"); plt.xlabel(\"Predicción\"); plt.ylabel(\"Real\"); plt.tight_layout()\n",
    "plt.savefig(OUT/\"m3_confusion_norm.png\", dpi=140); plt.show()\n",
    "\n",
    "pe = per_event[per_event[\"n\"]>=20]\n",
    "plt.figure(figsize=(8,4.2)); sns.barplot(data=pe.head(12), x=\"macro_f1\", y=\"event\"); plt.title(\"M3: Macro-F1 por evento (Top 12, n≥20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout(); plt.savefig(OUT/\"m3_event_top12.png\", dpi=140); plt.show()\n",
    "plt.figure(figsize=(8,4.2)); sns.barplot(data=pe.tail(12), x=\"macro_f1\", y=\"event\"); plt.title(\"M3: Macro-F1 por evento (Bottom 12, n≥20)\")\n",
    "plt.xlabel(\"Macro-F1\"); plt.ylabel(\"Evento\"); plt.tight_layout(); plt.savefig(OUT/\"m3_event_bottom12.png\", dpi=140); plt.show()\n",
    "\n",
    "pd.DataFrame([{\"model\":\"Zero-shot NLI (mDeBERTa)\",\"tau\":best_tau,\"f1_macro_test\":f1m,\"f1_weighted_test\":f1w}]).to_csv(OUT/\"summary.csv\", index=False)\n",
    "print(\"Figuras y tablas en:\", OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTefxkIvbkUe"
   },
   "source": [
    "## Conclusión — Modelo 3 (Zero-shot NLI con τ ajustado)\n",
    "\n",
    "**Resumen (dev → test)**  \n",
    "- τ óptimo en *dev* = **0.35** → **F1-macro(dev) = 0.372**.  \n",
    "- En *test* cae a **F1-macro = 0.156** (weighted ≈ 0.280; accuracy ≈ 0.268).\n",
    "\n",
    "**Lectura de los gráficos (test)**  \n",
    "- Mejoran algo **other** (F1≈0.40), **offensive** (≈0.27) y **joy** (≈0.26).  \n",
    "- **anger/sadness** siguen bajos y **fear/surprise/disgust** casi nulos.  \n",
    "- Matriz de confusión: fuerte deriva hacia **other** y **joy**; el modelo zero-shot no capta bien señales sutiles y sarcasmo.\n",
    "\n",
    "**Por qué pasa**  \n",
    "- **Desfase de distribución** dev→test: el τ afinado en dev no generaliza.  \n",
    "- **Verbalizadores** (“alegría”, “enojo”, …) y el marco NLI no están optimizados para emociones; sin fine-tuning, el modelo apuesta por clases frecuentes.  \n",
    "- **Clases raras** con muy bajo soporte → alta varianza y F1≈0.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Comparativa final (test)\n",
    "\n",
    "| Modelo                           | F1-macro | F1-weighted | Accuracy |\n",
    "|----------------------------------|:-------:|:-----------:|:--------:|\n",
    "| M1 — TF-IDF + LinearSVC          | **0.193** | 0.355       | 0.367    |\n",
    "| M2 — BETO (fine-tuning)          | **0.217** | 0.347       | 0.336    |\n",
    "| M3 — Zero-shot NLI (τ=0.35)      | **0.156** | 0.280       | 0.268    |\n",
    "\n",
    "**Conclusión global**  \n",
    "El baseline lineal (M1) y el zero-shot (M3) sufren con el desbalance y la sutileza de las emociones; el **fine-tuning con BETO (M2)** es el que mejor transfiere, aunque todavía necesita **balanceo de clases, pérdida focal y señal de evento en el input** para subir el macro-F1, especialmente en `fear/disgust/surprise`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0ebMmfYUrxL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
